# 3.11 混合精度训练

在上一节中，我们介绍了U-Net的构建和实现，了解了它的编码-解码结构和跳跃连接的作用。但是，U-Net的训练还有一个重要的问题，那就是如何提高训练效率和稳定性。在训练大规模的神经网络时，我们通常会遇到内存不足、计算速度慢、数值溢出等问题，这些问题会影响模型的收敛和性能。为了解决这些问题，一种常用的方法是使用混合精度训练，即在训练过程中使用不同的浮点数格式，如bf16和fp16，来减少内存占用和加速计算，同时保持较高的精度和范围。在本节中，我们将详细介绍混合精度训练的原理。

通过本文你将学习到：
- 什么是混合精度训练
- bf16和fp16的区别
- 混合精度训练的实现

## 3.11.1 混合精度训练简介

混合精度训练是一种在深度学习模型训练中同时使用不同的浮点数格式，及16位和32位，来提高训练效率和性能的方法。混合精度训练的主要优势是可以减少内存占用和计算时间，从而使模型可以使用更大的批量大小或更复杂的网络结构。

<div align=center><img src="./figures/float.png" ></div>

如果我们将单精度全部替换成半精度则会造成数据溢出和舍入误差。

### 数据溢出
对比它们的动态范围：
```
fp16（5.96E−8~ 65504）

fp32（1.4E-45 ~ 3.40E38)

bf16（9.2E−41~3.38E38)
```
可以看到，半精度跟单精度比，数据范围小了1000倍，如果将单精度全部替换成半精度容易出现数据的上溢和下溢。其中在模型训练过程中，更有可能出现的是过小的梯度导致数据下溢。

### 舍入误差
这三种精度它们的“间隔单位”也是不一样的，我们计算的最小正值就是它们的间隔单位，而间隔单位的大小决定了一个小值是否会被舍弃。比如说fp32和bf16虽然有大致一样的取值范围，但是它们的精度（间隔单位）是不一样的，当一个fp32的值+1.4E-45时，这个小值会被看到，原值会发生变动，但如果是一个bf16的值+1.4E-45，由于bf16的间隔单位为9.2E−41，这个小值就会被舍弃，原值不发生变动，这也就导致了舍入误差。

所以我们采用混合精度训练，以此来避免上述的问题出现。
### 原理
利用不同的浮点数格式的特点，将模型的不同部分分配给不同的精度。一般来说，模型的权重和梯度使用16位的半精度（float16）或者bfloat16来存储和计算，以节省内存和加速运算。但是，为了防止数值不稳定，模型的某些部分，如损失函数和权重更新，仍然使用32位的单精度（float32）来进行。这样，既可以利用16位的性能优势，又可以保持32位的数值稳定性

## 3.11.2 bf16和fp16

bf16和fp16都是16位的浮点数格式，但是它们的指数位和小数位的分配不同。bf16用8位表示指数，7位表示小数；fp16用5位表示指数，10位表示小数。

bf16和fp16的适用场景不同。bf16可以表示的整数范围更广泛，但是小数部分的精度较低；fp16可以表示的小数范围更广泛，但是整数部分的范围较小。因此，bf16更适合于需要处理大量整数的场景，如自然语言处理；fp16更适合于需要处理大量小数的场景，如计算机视觉。

## 混合精度训练实现

在pytorch中我们可以使用`torch.cuda.amp`（Automatic Mixed Precision，自动混合精度）模块来实现
```python
from torch.cuda.amp import autocast, GradScaler
# 在默认精度下创建模型和优化器
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

# 在训练开始时创建一个GradScaler
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()

        # 使用autocasting进行前向传播
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # 缩放损失。在缩放的损失上调用backward()以创建缩放的梯度。
        # 不建议在autocast下运行backward操作。
        # 在相应的前向操作中，backward操作以与autocast选择的dtype相同的dtype运行。
        scaler.scale(loss).backward()

        # scaler.step()首先对优化器的指定参数的梯度进行解调。
        # 如果这些梯度不包含无穷大或NaN，则调用optimizer.step()，
        # 否则跳过optimizer.step()。
        scaler.step(optimizer)

        # 更新下一次迭代的缩放因子。
        scaler.update()

```

- `autocast`: 控制上下文中的自动混合精度。在`with autocast`块内，模型前向和反向传播的计算将以指定的浮点数类型进行，这里是torch.float16，即fp16。

- `GradScaler`: 用于缩放梯度，以防止在fp16精度下出现梯度下溢问题。`scale`方法用于缩放损失。

## 参考资料
1. [torch.Tensor.bfloat16](https://pytorch.org/docs/stable/generated/torch.Tensor.bfloat16.html)
2. [amp_examples](https://pytorch.org/docs/stable/notes/amp_examples.html)
   